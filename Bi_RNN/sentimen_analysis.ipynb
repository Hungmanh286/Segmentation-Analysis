{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10165506,"sourceType":"datasetVersion","datasetId":6277509},{"sourceId":10177076,"sourceType":"datasetVersion","datasetId":6285972},{"sourceId":10186869,"sourceType":"datasetVersion","datasetId":6293322},{"sourceId":10191390,"sourceType":"datasetVersion","datasetId":6296767}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install underthesea","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T13:50:09.651010Z","iopub.execute_input":"2024-12-14T13:50:09.651513Z","iopub.status.idle":"2024-12-14T13:50:17.853980Z","shell.execute_reply.started":"2024-12-14T13:50:09.651464Z","shell.execute_reply":"2024-12-14T13:50:17.852812Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: underthesea in /opt/conda/lib/python3.10/site-packages (6.8.4)\nRequirement already satisfied: Click>=6.0 in /opt/conda/lib/python3.10/site-packages (from underthesea) (8.1.7)\nRequirement already satisfied: python-crfsuite>=0.9.6 in /opt/conda/lib/python3.10/site-packages (from underthesea) (0.9.11)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from underthesea) (3.2.4)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from underthesea) (4.66.4)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from underthesea) (2.32.3)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from underthesea) (1.4.2)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from underthesea) (1.2.2)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from underthesea) (6.0.2)\nRequirement already satisfied: underthesea-core==1.0.4 in /opt/conda/lib/python3.10/site-packages (from underthesea) (1.0.4)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk->underthesea) (1.16.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->underthesea) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->underthesea) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->underthesea) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->underthesea) (2024.6.2)\nRequirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->underthesea) (1.26.4)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->underthesea) (1.14.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->underthesea) (3.5.0)\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"from underthesea import word_tokenize\n\nsentence = \"Với cộng đồng người Bách Việt trước đây, việc thuần hóa mèo cũng có thể theo cách thức như vậy.\"\nprint(word_tokenize(sentence))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T13:50:17.856200Z","iopub.execute_input":"2024-12-14T13:50:17.856969Z","iopub.status.idle":"2024-12-14T13:50:18.116143Z","shell.execute_reply.started":"2024-12-14T13:50:17.856926Z","shell.execute_reply":"2024-12-14T13:50:18.115215Z"}},"outputs":[{"name":"stdout","text":"['Với', 'cộng đồng', 'người', 'Bách Việt', 'trước đây', ',', 'việc', 'thuần hóa', 'mèo', 'cũng', 'có thể', 'theo', 'cách thức', 'như vậy', '.']\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"import torch\ndef load_word_embeddings(file_path, embedding_dim=100, unk_init=torch.Tensor.normal_):\n    word_embeddings = {}\n    skipped_lines = 0\n\n    with open(file_path, 'r', encoding='utf-8') as f:\n        for line in f:\n            parts = line.strip().split()\n            if len(parts) != embedding_dim + 1:\n                skipped_lines += 1\n                continue  # Bỏ qua dòng không hợp lệ\n            try:\n                word = parts[0]\n                vector = torch.tensor(list(map(float, parts[1:])), dtype=torch.float)\n                word_embeddings[word] = vector\n            except ValueError:\n                skipped_lines += 1\n                continue  # Bỏ qua nếu không thể chuyển đổi thành float\n\n    # Tạo ma trận embedding\n    vocab_size = len(word_embeddings)\n    embedding_matrix = torch.empty(vocab_size, embedding_dim)\n\n    for i, (word, vector) in enumerate(word_embeddings.items()):\n        embedding_matrix[i] = vector\n\n    return word_embeddings, embedding_matrix\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T13:50:18.117471Z","iopub.execute_input":"2024-12-14T13:50:18.117860Z","iopub.status.idle":"2024-12-14T13:50:18.124968Z","shell.execute_reply.started":"2024-12-14T13:50:18.117820Z","shell.execute_reply":"2024-12-14T13:50:18.123973Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"# Đường dẫn tới file vi_word2vec.txt\nfile_path = \"/kaggle/input/dataset/vi_word2vec.txt\"\n\n# Gọi hàm để tải embeddings\nword_embeddings, embedding_matrix = load_word_embeddings(file_path, embedding_dim=100)\n\n# Kiểm tra kích thước của ma trận embedding\nprint(embedding_matrix.shape)  # >> torch.Size([số từ hợp lệ, 100])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T13:50:18.126072Z","iopub.execute_input":"2024-12-14T13:50:18.126356Z","iopub.status.idle":"2024-12-14T13:51:22.371076Z","shell.execute_reply.started":"2024-12-14T13:50:18.126330Z","shell.execute_reply":"2024-12-14T13:51:22.370108Z"}},"outputs":[{"name":"stdout","text":"torch.Size([1585076, 100])\n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"## Sử dụng pretrain để tạo bộ vocab","metadata":{}},{"cell_type":"code","source":"from itertools import chain\nfrom collections import Counter\n\nimport torch\nfrom tqdm import tqdm\nfrom underthesea import word_tokenize\n\n\nclass Vocabulary:\n    \"\"\" The Vocabulary class is used to record words, which are used to convert \n        text to numbers and vice versa.\n    \"\"\"\n\n    def __init__(self):\n        self.word2id = dict()\n        self.word2id['<pad>'] = 0   # Pad Token\n        self.word2id['<unk>'] = 1   # Unknown Token\n        self.unk_id = self.word2id['<unk>']\n        self.id2word = {v: k for k, v in self.word2id.items()}\n\n    def __getitem__(self, word):\n        return self.word2id.get(word, self.unk_id)\n\n    def __contains__(self, word):\n        return word in self.word2id \n    \n    def __len__(self):\n        return len(self.word2id)\n\n    def id2word(self, word_index):\n        \"\"\"\n        @param word_index (int)\n        @return word (str)\n        \"\"\"\n        return self.id2word[word_index]\n\n    def add(self, word):\n        \"\"\" Add word to vocabulary\n        @param word (str)\n        @return index (str): index of the word just added\n        \"\"\"\n        if word not in self:\n            word_index = self.word2id[word] = len(self.word2id)\n            self.id2word[word_index] = word\n            return word_index\n        else:\n            return self[word]\n\n    @staticmethod\n    def tokenize_corpus(corpus):\n        \"\"\"Split the documents of the corpus into words\n        @param corpus (list(str)): list of documents\n        @return tokenized_corpus (list(list(str))): list of words\n        \"\"\"\n        print(\"Tokenize the corpus...\")\n        tokenized_corpus = list()\n        for document in tqdm(corpus):\n            tokenized_document = [word.replace(\" \", \"_\") for word in word_tokenize(document)]\n            tokenized_corpus.append(tokenized_document)\n\n        return tokenized_corpus\n\n    def corpus_to_tensor(self, corpus, is_tokenized=False):\n        \"\"\" Convert corpus to a list of indices tensor\n        @param corpus (list(str) if is_tokenized==False else list(list(str)))\n        @param is_tokenized (bool)\n        @return indicies_corpus (list(tensor))\n        \"\"\"\n        if is_tokenized:\n            tokenized_corpus = corpus\n        else:\n            tokenized_corpus = self.tokenize_corpus(corpus)\n        indicies_corpus = list()\n        for document in tqdm(tokenized_corpus):\n            indicies_document = torch.tensor(list(map(lambda word: self[word], document)),\n                                             dtype=torch.int64)\n            indicies_corpus.append(indicies_document)\n\n        return indicies_corpus\n\n    def tensor_to_corpus(self, tensor):\n        \"\"\" Convert list of indices tensor to a list of tokenized documents\n        @param indicies_corpus (list(tensor))\n        @return corpus (list(list(str)))\n        \"\"\"\n        corpus = list()\n        for indicies in tqdm(tensor):\n            document = list(map(lambda index: self.id2word[index.item()], indicies))\n            corpus.append(document)\n\n        return corpus\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T13:51:22.380470Z","iopub.execute_input":"2024-12-14T13:51:22.380775Z","iopub.status.idle":"2024-12-14T13:51:22.439474Z","shell.execute_reply.started":"2024-12-14T13:51:22.380729Z","shell.execute_reply":"2024-12-14T13:51:22.438746Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"vocab = Vocabulary()\n# create vocabulary from pretrained word2vec\nwords_list = list(word_embeddings.keys())\nfor word in words_list:\n    vocab.add(word)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T13:51:22.440422Z","iopub.execute_input":"2024-12-14T13:51:22.440665Z","iopub.status.idle":"2024-12-14T13:51:23.627202Z","shell.execute_reply.started":"2024-12-14T13:51:22.440642Z","shell.execute_reply":"2024-12-14T13:51:23.626500Z"}},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":"## Load Data","metadata":{}},{"cell_type":"code","source":"from scipy.linalg import dft\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset\n\n\nclass LoadDataset(Dataset):\n    \"\"\" Load dataset from file csv\"\"\"\n\n    def __init__(self, vocab, csv_fpath=None, tokenized_fpath=None):\n        \"\"\"\n        @param vocab (Vocabulary)\n        @param csv_fpath (str)\n        @param tokenized_fpath (str)\n        \"\"\"\n        self.vocab = vocab\n        self.pad_idx = vocab[\"<pad>\"]\n        df = pd.read_csv(csv_fpath)\n        self.sentiments_list = list(df.sentiment)\n        self.reviews_list = list(df.vi_review)\n\n        sentiments_type = list(set(self.sentiments_list))\n        sentiments_type.sort()\n\n        self.sentiment2id = {sentiment: i for i, sentiment in enumerate(sentiments_type)}\n\n        if tokenized_fpath:\n            self.tokenized_reviews = torch.load(tokenized_fpath)\n        else:\n            self.tokenized_reviews = self.vocab.tokenize_corpus(self.reviews_list)\n\n        self.tensor_data = self.vocab.corpus_to_tensor(self.tokenized_reviews, is_tokenized=True)\n        self.tensor_label = torch.tensor([self.sentiment2id[sentiment] for sentiment in self.sentiments_list],\n                                         dtype=torch.float64)\n\n    def __len__(self):\n        return len(self.tensor_data)\n\n    def __getitem__(self, idx):\n        return self.tensor_data[idx], self.tensor_label[idx]\n\n    def collate_fn(self, examples):\n        examples = sorted(examples, key=lambda e: len(e[0]), reverse=True)\n\n        reviews = [e[0] for e in examples]\n        reviews = torch.nn.utils.rnn.pad_sequence(reviews,\n                                                  batch_first=False,\n                                                  padding_value=self.pad_idx)\n        reviews_lengths = torch.tensor([len(e[0]) for e in examples])\n        sentiments = torch.tensor([e[1] for e in examples])\n\n        return {\"reviews\": (reviews, reviews_lengths), \"sentiments\": sentiments}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T13:51:23.628206Z","iopub.execute_input":"2024-12-14T13:51:23.628457Z","iopub.status.idle":"2024-12-14T13:51:23.637946Z","shell.execute_reply.started":"2024-12-14T13:51:23.628432Z","shell.execute_reply":"2024-12-14T13:51:23.637060Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"dataset = LoadDataset(vocab, \"/kaggle/input/dataset/VI_IMDB.csv\", \"/kaggle/input/models/tokenized.pt\")\n# dataset = LoadDataset(vocab, \"/kaggle/input/dataset/VI_IMDB.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T13:51:23.639152Z","iopub.execute_input":"2024-12-14T13:51:23.639422Z","iopub.status.idle":"2024-12-14T13:51:40.688646Z","shell.execute_reply.started":"2024-12-14T13:51:23.639398Z","shell.execute_reply":"2024-12-14T13:51:40.687687Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_23/566285886.py:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  self.tokenized_reviews = torch.load(tokenized_fpath)\n100%|██████████| 50000/50000 [00:08<00:00, 5996.17it/s]\n","output_type":"stream"}],"execution_count":32},{"cell_type":"markdown","source":"## Split data","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import random_split\n\n\nsplit_rate = 0.8\nfull_size = len(dataset)\ntrain_size = (int)(split_rate * full_size)\nvalid_size = (int)((full_size - train_size)/2)\ntest_size = full_size - train_size - valid_size\ntrain_dataset, valid_dataset, test_dataset = random_split(dataset,\n                                                          lengths=[train_size, valid_size, test_size])\n\nlen(train_dataset), len(valid_dataset), len(test_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T13:51:40.694264Z","iopub.execute_input":"2024-12-14T13:51:40.694512Z","iopub.status.idle":"2024-12-14T13:51:40.705403Z","shell.execute_reply.started":"2024-12-14T13:51:40.694489Z","shell.execute_reply":"2024-12-14T13:51:40.704573Z"}},"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"(40000, 5000, 5000)"},"metadata":{}}],"execution_count":33},{"cell_type":"markdown","source":"## Recurrent Neural Network model","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\n\n\nclass RNN(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers,\n                 bidirectional, dropout, pad_idx):\n        \"\"\"\n        @param vocab_size (int)\n        @param embedding_dim (int)\n        @param hidden_dim (int)\n        @param n_layers (int)\n        @param bidirectional (bool)\n        @param dropout (float)\n        @param pad_idx (int)\n        \"\"\"\n        super().__init__()\n\n        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n\n        self.rnn = nn.LSTM(embedding_dim,\n                           hidden_dim,\n                           num_layers=n_layers,\n                           bidirectional=bidirectional,\n                           dropout=dropout)\n\n        self.fc = nn.Linear(hidden_dim * 2, 1)\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, text, text_lengths):\n        \"\"\"\n        @param text (torch.Tensor): shape = [sent len, batch size]\n        @param text_lengths (torch.Tensor): shape = [batch size]\n        @return\n        \"\"\"\n        #text = [sent len, batch size]\n        embedded = self.dropout(self.embedding(text))\n\n        #embedded = [sent len, batch size, emb dim]\n\n        #pack sequence\n        # lengths need to be on CPU!\n        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.to('cpu'))\n\n        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n\n        #unpack sequence\n        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n\n        #output = [sent len, batch size, hid dim * num directions]\n        #output over padding tokens are zero tensors\n\n        #hidden = [num layers * num directions, batch size, hid dim]\n        #cell = [num layers * num directions, batch size, hid dim]\n\n        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n        #and apply dropout\n\n        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n\n        #hidden = [batch size, hid dim * num directions]\n\n        return self.fc(hidden)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T13:51:40.706559Z","iopub.execute_input":"2024-12-14T13:51:40.706874Z","iopub.status.idle":"2024-12-14T13:51:40.714663Z","shell.execute_reply.started":"2024-12-14T13:51:40.706848Z","shell.execute_reply":"2024-12-14T13:51:40.713994Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\n\nBATCH_SIZE = 64\ntrain_dataloader = DataLoader(train_dataset,\n                              batch_size=BATCH_SIZE,\n                              shuffle=True,\n                              collate_fn=dataset.collate_fn)\nvalid_dataloader = DataLoader(valid_dataset,\n                              batch_size=BATCH_SIZE,\n                              shuffle=True,\n                              collate_fn=dataset.collate_fn)\ntest_dataloader = DataLoader(test_dataset,\n                              batch_size=BATCH_SIZE,\n                              shuffle=False,\n                              collate_fn=dataset.collate_fn)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T13:51:40.715396Z","iopub.execute_input":"2024-12-14T13:51:40.715659Z","iopub.status.idle":"2024-12-14T13:51:41.715602Z","shell.execute_reply.started":"2024-12-14T13:51:40.715631Z","shell.execute_reply":"2024-12-14T13:51:41.714667Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"INPUT_DIM = embedding_matrix.shape[0]\nEMBEDDING_DIM = 100\nBATCH_SIZE = 100\nHIDDEN_DIM = 256\nN_LAYERS = 3\nBIDIRECTIONAL = True\nDROPOUT = 0.5\nPAD_IDX = vocab[\"<pad>\"]\nUNK_IDX = vocab[\"<unk>\"]\n\nmodel = RNN(INPUT_DIM,\n            EMBEDDING_DIM,\n            HIDDEN_DIM,\n            N_LAYERS,\n            BIDIRECTIONAL,\n            DROPOUT,\n            PAD_IDX)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T13:51:41.723525Z","iopub.execute_input":"2024-12-14T13:51:41.723835Z","iopub.status.idle":"2024-12-14T13:51:43.352343Z","shell.execute_reply.started":"2024-12-14T13:51:41.723809Z","shell.execute_reply":"2024-12-14T13:51:43.351619Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"model.embedding.weight.data.copy_(embedding_matrix)\nmodel.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\nmodel.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T13:51:43.353449Z","iopub.execute_input":"2024-12-14T13:51:43.353826Z","iopub.status.idle":"2024-12-14T13:51:43.457579Z","shell.execute_reply.started":"2024-12-14T13:51:43.353787Z","shell.execute_reply":"2024-12-14T13:51:43.456726Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"import torch\nimport torch.optim as optim\nimport torch.nn as nn\n\nlearning_rate = 0.1\nmomentum = 0.9 \n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Khởi tạo optimizer với learning rate và momentum\noptimizer = optim.SGD(model.parameters(), lr = learning_rate, momentum=momentum)\n\n# Khởi tạo loss function\ncriterion = nn.BCEWithLogitsLoss().to(device)\n\n# Chuyển model sang device\nmodel = model.to(device)\n\n# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# optimizer = optim.Adam(model.parameters())\n\n# criterion = nn.BCEWithLogitsLoss().to(device)\n\n# model = model.to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T13:51:43.461230Z","iopub.execute_input":"2024-12-14T13:51:43.461485Z","iopub.status.idle":"2024-12-14T13:51:44.383154Z","shell.execute_reply.started":"2024-12-14T13:51:43.461460Z","shell.execute_reply":"2024-12-14T13:51:44.382180Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"def count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f'The model has {count_parameters(model):,} trainable parameters')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T13:51:44.384303Z","iopub.execute_input":"2024-12-14T13:51:44.384741Z","iopub.status.idle":"2024-12-14T13:51:44.389991Z","shell.execute_reply.started":"2024-12-14T13:51:44.384713Z","shell.execute_reply":"2024-12-14T13:51:44.389045Z"}},"outputs":[{"name":"stdout","text":"The model has 162,395,217 trainable parameters\n","output_type":"stream"}],"execution_count":39},{"cell_type":"markdown","source":"## Train model","metadata":{}},{"cell_type":"code","source":"def binary_accuracy(preds, y):\n    \"\"\"\n    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n    @param preds (torch.Tensor): shape = [batch_size]\n    @param y (torch.Tensor): shape = [batch_size]\n    @return acc (torch.Tensor): shape = [1]\n    \"\"\"\n    #round predictions to the closest integer\n    rounded_preds = torch.round(torch.sigmoid(preds))\n    correct = (rounded_preds == y).float() #convert into float for division\n    acc = correct.sum() / len(correct)\n    return acc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T13:51:44.390998Z","iopub.execute_input":"2024-12-14T13:51:44.391331Z","iopub.status.idle":"2024-12-14T13:51:44.399082Z","shell.execute_reply.started":"2024-12-14T13:51:44.391289Z","shell.execute_reply":"2024-12-14T13:51:44.398332Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"def train(model, dataloader, optimizer, criterion, device):\n    \"\"\"\n    @param model (RNN)\n    @param dataloader (DataLoader)\n    @param optimizer (torch.optim)\n    @param criterion (torch.nn.modules.loss)\n    @param device (torch.device)\n    @return epoch_loss (float): model's loss of this epoch\n    @return epoch_acc (float): model's accuracy of this epoch\n    \"\"\"\n    epoch_loss = 0\n    epoch_acc = 0\n\n    model.train()\n\n    for batch in dataloader:\n\n        optimizer.zero_grad()\n        reviews, reviews_lengths = batch[\"reviews\"]\n        reviews = reviews.to(device)\n        predictions = model(reviews, reviews_lengths).squeeze(1)\n        sentiments = batch[\"sentiments\"].to(device)\n        loss = criterion(predictions, sentiments)\n        acc = binary_accuracy(predictions, sentiments)\n\n        loss.backward()\n        optimizer.step()\n\n        epoch_loss += loss.item()\n        epoch_acc += acc.item()\n\n    batch_num = len(dataloader)\n    return epoch_loss / batch_num, epoch_acc / batch_num","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T13:51:44.400250Z","iopub.execute_input":"2024-12-14T13:51:44.400795Z","iopub.status.idle":"2024-12-14T13:51:44.410291Z","shell.execute_reply.started":"2024-12-14T13:51:44.400739Z","shell.execute_reply":"2024-12-14T13:51:44.409565Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"def evaluate(model, dataloader, criterion, device):\n    \"\"\"\n    @param model (RNN)\n    @param dataloader (DataLoader)\n    @param criterion (torch.nn.modules.loss)\n    @param device (torch.device)\n    @return epoch_loss (float): model's loss of this epoch\n    @return epoch_acc (float): model's accuracy of this epoch\n    \"\"\"\n    epoch_loss = 0\n    epoch_acc = 0\n\n    model.eval()\n\n    with torch.no_grad():\n        for batch in dataloader:\n\n            reviews, reviews_lengths = batch[\"reviews\"]\n            reviews = reviews.to(device)\n            predictions = model(reviews, reviews_lengths).squeeze(1)\n\n            sentiments = batch[\"sentiments\"].to(device)\n            loss = criterion(predictions, sentiments)\n            acc = binary_accuracy(predictions, sentiments)\n\n            epoch_loss += loss.item()\n            epoch_acc += acc.item()\n\n    batch_num = len(dataloader)\n    return epoch_loss / batch_num, epoch_acc / batch_num","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T13:51:44.411343Z","iopub.execute_input":"2024-12-14T13:51:44.412175Z","iopub.status.idle":"2024-12-14T13:51:44.424612Z","shell.execute_reply.started":"2024-12-14T13:51:44.412123Z","shell.execute_reply":"2024-12-14T13:51:44.423959Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"import time\n\ndef epoch_time(start_time, end_time):\n    elapsed_time = end_time - start_time\n    elapsed_mins = int(elapsed_time / 60)\n    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n    return elapsed_mins, elapsed_secs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T13:51:44.425491Z","iopub.execute_input":"2024-12-14T13:51:44.425718Z","iopub.status.idle":"2024-12-14T13:51:44.433320Z","shell.execute_reply.started":"2024-12-14T13:51:44.425695Z","shell.execute_reply":"2024-12-14T13:51:44.432637Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"N_EPOCHS = 5\n\nbest_valid_loss = float(\"inf\")\n\nfor epoch in range(N_EPOCHS):\n\n    start_time = time.time()\n\n    train_loss, train_acc = train(model, train_dataloader, optimizer, criterion, device)\n    valid_loss, valid_acc = evaluate(model, valid_dataloader, criterion, device)\n\n    end_time = time.time()\n\n    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n\n    if valid_loss < best_valid_loss:\n        best_valid_loss = valid_loss\n        torch.save(model.state_dict(), \"model_100_3l_01.pt\")\n\n    print(f\"Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s\")\n    print(f\"\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%\")\n    print(f\"\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Evaluation","metadata":{}},{"cell_type":"code","source":"INPUT_DIM = embedding_matrix.shape[0]\nEMBEDDING_DIM = 100 \nBATCH_SIZE = 100\nHIDDEN_DIM = 256\nN_LAYERS = 3\nBIDIRECTIONAL = True\nDROPOUT = 0.5\nPAD_IDX = vocab[\"<pad>\"]\nUNK_IDX = vocab[\"<unk>\"]\n\nmodel = RNN(INPUT_DIM,\n            EMBEDDING_DIM,\n            HIDDEN_DIM,\n            N_LAYERS,\n            BIDIRECTIONAL,\n            DROPOUT,\n            PAD_IDX)\n\n\n# Load the state_dict into the model\ncheckpoint = torch.load('/kaggle/input/three-layer/model_64_3l_adam.pt')\nmodel.load_state_dict(checkpoint)\n\n# Switch the model to evaluation model\nmodel.eval()\n\n# Move the model to the appropriate device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T13:52:28.129044Z","iopub.execute_input":"2024-12-14T13:52:28.129398Z","iopub.status.idle":"2024-12-14T13:52:30.751869Z","shell.execute_reply.started":"2024-12-14T13:52:28.129368Z","shell.execute_reply":"2024-12-14T13:52:30.750941Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_23/3746140195.py:21: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load('/kaggle/input/three-layer/model_64_3l_adam.pt')\n","output_type":"stream"},{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"RNN(\n  (embedding): Embedding(1585076, 100, padding_idx=0)\n  (rnn): LSTM(100, 256, num_layers=3, dropout=0.5, bidirectional=True)\n  (fc): Linear(in_features=512, out_features=1, bias=True)\n  (dropout): Dropout(p=0.5, inplace=False)\n)"},"metadata":{}}],"execution_count":45},{"cell_type":"code","source":"test_loss, test_acc = evaluate(model, test_dataloader, criterion, device)\nprint(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T13:52:33.919292Z","iopub.execute_input":"2024-12-14T13:52:33.920125Z","iopub.status.idle":"2024-12-14T13:52:41.292771Z","shell.execute_reply.started":"2024-12-14T13:52:33.920078Z","shell.execute_reply":"2024-12-14T13:52:41.291840Z"}},"outputs":[{"name":"stdout","text":"Test Loss: 0.148 | Test Acc: 94.84%\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"def predict_sentiment(model, sentence, vocab, device):\n    model.eval()\n    corpus = [sentence]\n    tensor = vocab.corpus_to_tensor(corpus)[0].to(device)\n    tensor = tensor.unsqueeze(1)\n    length = [len(tensor)]\n    length_tensor = torch.LongTensor(length)\n    prediction = torch.sigmoid(model(tensor, length_tensor))\n    return prediction.item()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T13:52:26.070323Z","iopub.status.idle":"2024-12-14T13:52:26.070621Z","shell.execute_reply.started":"2024-12-14T13:52:26.070480Z","shell.execute_reply":"2024-12-14T13:52:26.070495Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sentence = \"anh oi em khong co tiennn\"\n\npredict_sentiment(model, sentence, vocab, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T13:52:26.072374Z","iopub.status.idle":"2024-12-14T13:52:26.072828Z","shell.execute_reply.started":"2024-12-14T13:52:26.072583Z","shell.execute_reply":"2024-12-14T13:52:26.072605Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sentence = \"mê nhạc mê anhhhh\"\n\npredict_sentiment(model, sentence, vocab, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T13:52:26.074477Z","iopub.status.idle":"2024-12-14T13:52:26.074791Z","shell.execute_reply.started":"2024-12-14T13:52:26.074629Z","shell.execute_reply":"2024-12-14T13:52:26.074642Z"}},"outputs":[],"execution_count":null}]}